{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462fbd0e",
   "metadata": {},
   "source": [
    "# Libary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7392720-6ecc-41a7-ab47-4db083e3323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from spark_functions import create_SparkSession\n",
    "\n",
    "\n",
    "\n",
    "from logging_functions import inject_logging\n",
    "\n",
    "\n",
    "\n",
    "from string_functions import to_snake_case\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "\n",
    "\n",
    "spark = create_SparkSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30d9d3",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "## Camera Traffic Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81231ebd-55c8-46b7-9915-ffd7028a0bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+----------+----------+-------------+------+----------------------------+-------------+-----------------------+-----------------------+-----+---+----+----+------+-----------+--------------------+\n",
      "|           record_id|atd_device_id|           read_date| intersection_name| direction|  movement|heavy_vehicle|volume|speed_average_miles_per_hour|speed_std_dev|seconds_in_zone_average|seconds_in_zone_std_dev|month|day|year|hour|minute|day_of_week|bin_duration_seconds|\n",
      "+--------------------+-------------+--------------------+------------------+----------+----------+-------------+------+----------------------------+-------------+-----------------------+-----------------------+-----+---+----+----+------+-----------+--------------------+\n",
      "|f6d2caebed1dc902b...|         7047|03/11/2020 02:30:...|MANOR RD / 51ST ST|NORTHBOUND| LEFT TURN|        false|    14|                         8.5|         2.21|                 12.407|                 23.497|    3| 11|2020|  14|    30|          3|                 900|\n",
      "|5ceb46cceba70cbb9...|         7047|03/11/2020 02:30:...|MANOR RD / 51ST ST|NORTHBOUND|RIGHT TURN|        false|     6|                       9.167|        0.408|                  3.033|                  0.734|    3| 11|2020|  14|    30|          3|                 900|\n",
      "|a5610981d8ae35224...|         7047|03/11/2020 02:30:...|MANOR RD / 51ST ST|NORTHBOUND|      THRU|        false|    48|                      12.729|        3.625|                 18.598|                 32.737|    3| 11|2020|  14|    30|          3|                 900|\n",
      "|e707407844e917a07...|         7047|03/11/2020 02:30:...|MANOR RD / 51ST ST|SOUTHBOUND| LEFT TURN|        false|    20|                         9.6|        4.453|                  38.39|                 38.616|    3| 11|2020|  14|    30|          3|                 900|\n",
      "|50ac50f9f95266900...|         7047|03/11/2020 02:30:...|MANOR RD / 51ST ST|SOUTHBOUND|RIGHT TURN|        false|     7|                       7.143|        1.773|                   14.5|                 16.878|    3| 11|2020|  14|    30|          3|                 900|\n",
      "|7b70f41ec5ead5027...|         7047|03/11/2020 02:30:...|MANOR RD / 51ST ST|SOUTHBOUND|      THRU|        false|    33|                       15.97|        5.736|                  2.564|                 10.025|    3| 11|2020|  14|    30|          3|                 900|\n",
      "|c6ada4d05eafde48d...|         7047|03/11/2020 02:30:...|MANOR RD / 51ST ST|SOUTHBOUND|      THRU|         true|     5|                        18.8|        7.791|                  26.88|                  37.45|    3| 11|2020|  14|    30|          3|                 900|\n",
      "|663b83bbd8e2b1f36...|         7047|03/11/2020 02:30:...|MANOR RD / 51ST ST| WESTBOUND| LEFT TURN|        false|     4|                        8.25|          1.5|                 17.975|                 31.716|    3| 11|2020|  14|    30|          3|                 900|\n",
      "|f7717a8e45df4003d...|         7047|03/11/2020 02:30:...|MANOR RD / 51ST ST| WESTBOUND|RIGHT TURN|        false|     9|                       8.667|        2.236|                    3.4|                  2.228|    3| 11|2020|  14|    30|          3|                 900|\n",
      "|9320b0f9e5850015d...|         7047|03/11/2020 02:30:...|MANOR RD / 51ST ST| WESTBOUND|      THRU|        false|    76|                      16.303|        4.671|                  7.572|                 17.438|    3| 11|2020|  14|    30|          3|                 900|\n",
      "+--------------------+-------------+--------------------+------------------+----------+----------+-------------+------+----------------------------+-------------+-----------------------+-----------------------+-----+---+----+----+------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../data\"\n",
    "\n",
    "\n",
    "camera_traffic = spark.read.option(\"header\", True).csv(\n",
    "    f\"{base_path}/Camera_Traffic_Counts_20250125.csv\"\n",
    ")\n",
    "for col_ in camera_traffic.columns:\n",
    "\n",
    "    camera_traffic = camera_traffic.withColumnRenamed(col_, to_snake_case(col_))\n",
    "\n",
    "camera_traffic.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f011c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read_date    03/11/2020 02:30:00 PM\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera_traffic.limit(10).select(\"read_date\").toPandas().iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a67cfc",
   "metadata": {},
   "source": [
    "## Radar Traffic Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b532b088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------+--------------------+--------------------+--------+------+---------+-----+-----+---+----+----+------+-----------+--------+---------+\n",
      "|              row_id|detector_id|kits_id|           read_date|   intersection_name|    lane|volume|occupancy|speed|month|day|year|hour|minute|day_of_week|time_bin|direction|\n",
      "+--------------------+-----------+-------+--------------------+--------------------+--------+------+---------+-----+-----+---+----+----+------+-----------+--------+---------+\n",
      "|9794d3655e0572eb8...|         74|     19|01/24/2018 04:15:...|         KINNEYLAMAR|   Lane1|     4|        0|   18|    1| 23|2018|  22|    15|          2|   22:15|     None|\n",
      "|1f51f4e6a68297c9f...|         92|     24|12/17/2017 01:45:...|    LOOP 360LAKEWOOD|  NB_out|   103|        6|   40|   12| 16|2017|  19|    45|          6|   19:45|       NB|\n",
      "|9de27c4292306d305...|         10|      3|01/01/156489 12:0...|     LAMARSHOALCREEK|  SB_out|    20|        1|   33|    8|  3|2019|  23|    15|          6|   23:15|       SB|\n",
      "|b5a5739acbc9481c9...|          1|      1|04/03/2020 11:15:...|       LAMARMANCHACA|  NB_out|     8|        0|   32|    4|  3|2020|  23|    15|          5|   23:15|       NB|\n",
      "|8b4b5e8a088d964b3...|         42|     14|01/24/2018 03:45:...|Robert E LeeBarto...|      SB|    44|        2|   27|    1| 23|2018|  21|    45|          2|   21:45|       SB|\n",
      "|31dd37230aa53d251...|         43|     14|01/24/2018 03:45:...|Robert E LeeBarto...|      NB|    13|        0|   28|    1| 23|2018|  21|    45|          2|   21:45|       NB|\n",
      "|f9ae7785b78479b31...|         75|     19|01/24/2018 04:15:...|         KINNEYLAMAR|   Lane2|     0|        0|  149|    1| 23|2018|  22|    15|          2|   22:15|     None|\n",
      "|60c5f3a6afa691202...|         87|     23|01/24/2018 04:15:...|      BURNETPALM WAY|  SB_out|    26|        1|   41|    1| 23|2018|  22|    15|          2|   22:15|       SB|\n",
      "|02b9141602b86cccf...|         88|     23|01/24/2018 04:15:...|      BURNETPALM WAY|   SB_in|    31|        1|   41|    1| 23|2018|  22|    15|          2|   22:15|       SB|\n",
      "|7cfa1ed2734114d18...|         89|     23|01/24/2018 04:15:...|      BURNETPALM WAY|SB_Lturn|    12|        1|   18|    1| 23|2018|  22|    15|          2|   22:15|       SB|\n",
      "+--------------------+-----------+-------+--------------------+--------------------+--------+------+---------+-----+-----+---+----+----+------+-----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "radar_traffic = spark.read.option(\"header\", True).csv(\n",
    "    f\"{base_path}/Radar_Traffic_Counts_20250125.csv\"\n",
    ")\n",
    "for col_ in radar_traffic.columns:\n",
    "    radar_traffic = radar_traffic.withColumnRenamed(col_, to_snake_case(col_))\n",
    "\n",
    "# radar_traffic = radar_traffic.withColumn(\n",
    "#     \"read_date\", to_timestamp(col(\"read_date\"), \"MM/dd/yyyy hh:mm:ss a\")\n",
    "# )\n",
    "radar_traffic.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51f84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606de259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------+--------------------+-----------------+------+------+---------+-----+-----+---+----+----+------+-----------+--------+---------+-------------+\n",
      "|              row_id|detector_id|kits_id|           read_date|intersection_name|  lane|volume|occupancy|speed|month|day|year|hour|minute|day_of_week|time_bin|direction|is_valid_date|\n",
      "+--------------------+-----------+-------+--------------------+-----------------+------+------+---------+-----+-----+---+----+----+------+-----------+--------+---------+-------------+\n",
      "|9de27c4292306d305...|         10|      3|01/01/156489 12:0...|  LAMARSHOALCREEK|SB_out|    20|        1|   33|    8|  3|2019|  23|    15|          6|   23:15|       SB|        false|\n",
      "+--------------------+-----------+-------+--------------------+-----------------+------+------+---------+-----+-----+---+----+----+------+-----------+--------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, lit\n",
    "\n",
    "# Define regex pattern to find proper dates\n",
    "regex_pattern = r\"^\\\\d{2}/\\\\d{2}/\\\\d{4} \\\\d{2}:\\\\d{2}:\\\\d{2} [AP]M$\"\n",
    "\n",
    "# Add a new column for valid dates\n",
    "radar_traffic = radar_traffic.withColumn(\n",
    "    \"is_valid_date\", expr(f\"regexp_like(read_date, '{regex_pattern}')\")\n",
    ")\n",
    "\n",
    "radar_traffic.where(~col(\"is_valid_date\")).show()\n",
    "\n",
    "# Show results\n",
    "# radar_traffic.select(\"read_date\", \"is_valid_date\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "78657ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>detector_id</th>\n",
       "      <th>kits_id</th>\n",
       "      <th>read_date</th>\n",
       "      <th>intersection_name</th>\n",
       "      <th>lane</th>\n",
       "      <th>volume</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>speed</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>time_bin</th>\n",
       "      <th>direction</th>\n",
       "      <th>is_valid_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9042</th>\n",
       "      <td>96a9ecd4f3762ef2738d3893f13a29a5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>01/19/1970 02:38:11 AM</td>\n",
       "      <td>LAMARSHOALCREEK</td>\n",
       "      <td>SB_out</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>15:30</td>\n",
       "      <td>SB</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9043</th>\n",
       "      <td>e2da7f44124e7fa6d5d026ba53652a93</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>01/19/1970 02:38:12 AM</td>\n",
       "      <td>LAMARSHOALCREEK</td>\n",
       "      <td>SB_out</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>15:45</td>\n",
       "      <td>SB</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9044</th>\n",
       "      <td>4798e6b1afa66bc31b67145451879bd4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>01/19/1970 02:38:13 AM</td>\n",
       "      <td>LAMARSHOALCREEK</td>\n",
       "      <td>SB_out</td>\n",
       "      <td>143</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16:00</td>\n",
       "      <td>SB</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9045</th>\n",
       "      <td>abdca3d62f9eb286d4d256f9860fb038</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>01/19/1970 02:38:14 AM</td>\n",
       "      <td>LAMARSHOALCREEK</td>\n",
       "      <td>SB_out</td>\n",
       "      <td>177</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>16:15</td>\n",
       "      <td>SB</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9046</th>\n",
       "      <td>1b7d95a42ad061d3a0a9a1f57f152d66</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>01/19/1970 02:38:15 AM</td>\n",
       "      <td>LAMARSHOALCREEK</td>\n",
       "      <td>SB_out</td>\n",
       "      <td>139</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>16:45</td>\n",
       "      <td>SB</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11632</th>\n",
       "      <td>cc5947b5488e6a0f121710e8df42bb2f</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>01/19/1970 03:21:49 AM</td>\n",
       "      <td>LAMARSHOALCREEK</td>\n",
       "      <td>SB_out</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2019</td>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>22:45</td>\n",
       "      <td>SB</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11633</th>\n",
       "      <td>a0bc4043db39395fa08777909f838846</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>01/19/1970 03:21:50 AM</td>\n",
       "      <td>LAMARSHOALCREEK</td>\n",
       "      <td>SB_out</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2019</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23:00</td>\n",
       "      <td>SB</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11634</th>\n",
       "      <td>357797d70d3c2c6af27fccc910152f51</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>01/19/1970 03:21:51 AM</td>\n",
       "      <td>LAMARSHOALCREEK</td>\n",
       "      <td>SB_out</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2019</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>23:15</td>\n",
       "      <td>SB</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11635</th>\n",
       "      <td>2cd984316fe89402cd45543733b5bc37</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>01/19/1970 03:21:52 AM</td>\n",
       "      <td>LAMARSHOALCREEK</td>\n",
       "      <td>SB_out</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2019</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>23:30</td>\n",
       "      <td>SB</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11636</th>\n",
       "      <td>405aa62139af0c5c226c215f93c2729c</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>01/19/1970 03:21:53 AM</td>\n",
       "      <td>LAMARSHOALCREEK</td>\n",
       "      <td>SB_out</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2019</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>23:45</td>\n",
       "      <td>SB</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2595 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 row_id detector_id kits_id  \\\n",
       "9042   96a9ecd4f3762ef2738d3893f13a29a5          10       3   \n",
       "9043   e2da7f44124e7fa6d5d026ba53652a93          10       3   \n",
       "9044   4798e6b1afa66bc31b67145451879bd4          10       3   \n",
       "9045   abdca3d62f9eb286d4d256f9860fb038          10       3   \n",
       "9046   1b7d95a42ad061d3a0a9a1f57f152d66          10       3   \n",
       "...                                 ...         ...     ...   \n",
       "11632  cc5947b5488e6a0f121710e8df42bb2f          10       3   \n",
       "11633  a0bc4043db39395fa08777909f838846          10       3   \n",
       "11634  357797d70d3c2c6af27fccc910152f51          10       3   \n",
       "11635  2cd984316fe89402cd45543733b5bc37          10       3   \n",
       "11636  405aa62139af0c5c226c215f93c2729c          10       3   \n",
       "\n",
       "                    read_date intersection_name    lane volume occupancy  \\\n",
       "9042   01/19/1970 02:38:11 AM   LAMARSHOALCREEK  SB_out    150         8   \n",
       "9043   01/19/1970 02:38:12 AM   LAMARSHOALCREEK  SB_out    150         8   \n",
       "9044   01/19/1970 02:38:13 AM   LAMARSHOALCREEK  SB_out    143         7   \n",
       "9045   01/19/1970 02:38:14 AM   LAMARSHOALCREEK  SB_out    177         9   \n",
       "9046   01/19/1970 02:38:15 AM   LAMARSHOALCREEK  SB_out    139         7   \n",
       "...                       ...               ...     ...    ...       ...   \n",
       "11632  01/19/1970 03:21:49 AM   LAMARSHOALCREEK  SB_out     43         2   \n",
       "11633  01/19/1970 03:21:50 AM   LAMARSHOALCREEK  SB_out     41         2   \n",
       "11634  01/19/1970 03:21:51 AM   LAMARSHOALCREEK  SB_out     38         2   \n",
       "11635  01/19/1970 03:21:52 AM   LAMARSHOALCREEK  SB_out     39         2   \n",
       "11636  01/19/1970 03:21:53 AM   LAMARSHOALCREEK  SB_out     49         2   \n",
       "\n",
       "      speed month day  year hour minute day_of_week time_bin direction  \\\n",
       "9042     33     8   1  2019   15     31           4    15:30        SB   \n",
       "9043     33     8   1  2019   15     45           4    15:45        SB   \n",
       "9044     34     8   1  2019   16      0           4    16:00        SB   \n",
       "9045     34     8   1  2019   16     15           4    16:15        SB   \n",
       "9046      0     8   1  2019   16     45           4    16:45        SB   \n",
       "...     ...   ...  ..   ...  ...    ...         ...      ...       ...   \n",
       "11632    32     8  31  2019   22     45           6    22:45        SB   \n",
       "11633    32     8  31  2019   23      0           6    23:00        SB   \n",
       "11634    31     8  31  2019   23     15           6    23:15        SB   \n",
       "11635    33     8  31  2019   23     30           6    23:30        SB   \n",
       "11636    33     8  31  2019   23     45           6    23:45        SB   \n",
       "\n",
       "       is_valid_date  \n",
       "9042            True  \n",
       "9043            True  \n",
       "9044            True  \n",
       "9045            True  \n",
       "9046            True  \n",
       "...              ...  \n",
       "11632           True  \n",
       "11633           True  \n",
       "11634           True  \n",
       "11635           True  \n",
       "11636           True  \n",
       "\n",
       "[2595 rows x 18 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = (\n",
    "    radar_traffic.where((col(\"is_valid_date\")) & (col(\"detector_id\") == lit(\"10\")))\n",
    "    .sort(\"read_date\")\n",
    "    .toPandas()\n",
    ")\n",
    "temp[(temp.month == \"8\") & (temp.year == \"2019\" & (temp.hiour))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b31b89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/24/2018 04:15:01 AM: True\n",
      "12/17/2017 01:45:00 AM: True\n",
      "04/03/2020 11:15:00 PM: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regex_pattern = r\"^\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2} [AP]M$\"\n",
    "test_data = [\n",
    "    \"01/24/2018 04:15:01 AM\",\n",
    "    \"12/17/2017 01:45:00 AM\",\n",
    "    \"04/03/2020 11:15:00 PM\",\n",
    "]\n",
    "\n",
    "for date in test_data:\n",
    "    print(f\"{date}: {bool(re.match(regex_pattern, date))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9c052dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12/17/2017 01:45:00 AM'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = radar_traffic.select(\"read_date\").limit(10).toPandas()\n",
    "temp[\"read_date\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff676df1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o771.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 2420) (Bravo-PC.mshome.net executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '01/01/156489 12:00:00 AM' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.time.format.DateTimeParseException: Text '01/01/156489 12:00:00 AM' could not be parsed at index 6\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2106)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1934)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4324)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '01/01/156489 12:00:00 AM' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.time.format.DateTimeParseException: Text '01/01/156489 12:00:00 AM' could not be parsed at index 6\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2106)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1934)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\r\n\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m invalid_rows \u001b[38;5;241m=\u001b[39m radar_traffic\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;241m~\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_date\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrlike(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m [AP]M$\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43minvalid_rows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Repos\\austin_traffic_mobility_analysis\\.venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mg:\\Repos\\austin_traffic_mobility_analysis\\.venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:978\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    971\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    972\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    975\u001b[0m         },\n\u001b[0;32m    976\u001b[0m     )\n\u001b[1;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Repos\\austin_traffic_mobility_analysis\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mg:\\Repos\\austin_traffic_mobility_analysis\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mg:\\Repos\\austin_traffic_mobility_analysis\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o771.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 2420) (Bravo-PC.mshome.net executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '01/01/156489 12:00:00 AM' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.time.format.DateTimeParseException: Text '01/01/156489 12:00:00 AM' could not be parsed at index 6\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2106)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1934)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4324)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '01/01/156489 12:00:00 AM' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\r\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.time.format.DateTimeParseException: Text '01/01/156489 12:00:00 AM' could not be parsed at index 6\r\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2106)\r\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1934)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\r\n\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "invalid_rows = radar_traffic.filter(\n",
    "    ~col(\"read_date\").rlike(r\"^\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2} [AP]M$\")\n",
    ")\n",
    "invalid_rows.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ec533",
   "metadata": {},
   "source": [
    "## Traffic Detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aff05ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------+------------------+-----------------+--------------------+---------------+---------+--------------------+--------------------+--------------+------------------------+-----------------+------------------+--------------------+\n",
      "|detector_id|detector_type|detector_status|detector_direction|detector_movement|       location_name|atd_location_id|signal_id|        created_date|       modified_date|ip_comm_status|comm_status_datetime_utc|location_latitude|location_longitude|            location|\n",
      "+-----------+-------------+---------------+------------------+-----------------+--------------------+---------------+---------+--------------------+--------------------+--------------+------------------------+-----------------+------------------+--------------------+\n",
      "|        257|        VIDEO|             OK|        NORTHBOUND|        THRU ONLY| 12700 BLK DESSAU RD|   LOC16-004035|      820|                NULL|10/07/2024 09:31:...|          NULL|                    NULL|             NULL|              NULL|POINT (-97.642685...|\n",
      "|        652|         LOOP|        REMOVED|         WESTBOUND|        LEFT TURN| AIRPORT BLVD / 1...|   LOC16-000810|      163|                NULL|02/27/2024 09:12:...|          NULL|                    NULL|             NULL|              NULL|POINT (-97.700333...|\n",
      "|         44|         LOOP|             OK|         WESTBOUND|THRU / RIGHT TURN| LAMAR BLVD / MAR...|   LOC16-000545|      110|                NULL|12/22/2023 02:37:...|          NULL|                    NULL|             NULL|              NULL|POINT (-97.752075...|\n",
      "|       1707|         LOOP|             OK|         WESTBOUND|THRU / RIGHT TURN| BURNET RD / ADAM...|   LOC16-004710|      964|06/28/2017 12:00:...|12/18/2024 05:17:...|          NULL|                    NULL|             NULL|              NULL|POINT (-97.739891...|\n",
      "|         71|         LOOP|        REMOVED|         WESTBOUND|THRU / RIGHT TURN| WILLIAM CANNON D...|   LOC16-002025|      413|                NULL|06/27/2020 10:46:...|          NULL|                    NULL|             NULL|              NULL|POINT (-97.770111...|\n",
      "|        119|         LOOP|        REMOVED|        NORTHBOUND|THRU / RIGHT TURN| MANOR RD / ED BL...|   LOC16-003095|      627|                NULL|06/27/2020 04:09:...|          NULL|                    NULL|             NULL|              NULL|POINT (-97.663574...|\n",
      "|       2652|         LOOP|        REMOVED|         WESTBOUND|THRU / RIGHT TURN| AVERY RANCH BLVD...|   LOC16-003995|      812|06/23/2017 12:00:...|06/20/2022 07:08:...|          NULL|                    NULL|             NULL|              NULL|POINT (-97.767067...|\n",
      "|       1800|         LOOP|        REMOVED|         WESTBOUND|THRU / RIGHT TURN| WILLIAM CANNON D...|   LOC16-004620|      946|06/28/2017 12:00:...|12/17/2023 07:24:...|          NULL|                    NULL|             NULL|              NULL|POINT (-97.836968...|\n",
      "|        202|        VIDEO|             OK|        SOUTHBOUND|        LEFT TURN| CONGRESS AVE / L...|   LOC16-001140|      232|                NULL|04/05/2024 05:37:...|          NULL|                    NULL|             NULL|              NULL|POINT (-97.760208...|\n",
      "|        834|         LOOP|             OK|        SOUTHBOUND|THRU / RIGHT TURN| HOWARD LN / CENT...|   LOC16-001755|      358|                NULL|12/10/2023 08:22:...|          NULL|                    NULL|             NULL|              NULL|POINT (-97.665825...|\n",
      "+-----------+-------------+---------------+------------------+-----------------+--------------------+---------------+---------+--------------------+--------------------+--------------+------------------------+-----------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "traffic_detectors = spark.read.option(\"header\", True).csv(\n",
    "    f\"{base_path}/Traffic_Detectors_20250125.csv\"\n",
    ")\n",
    "for col_ in traffic_detectors.columns:\n",
    "    traffic_detectors = traffic_detectors.withColumnRenamed(col_, to_snake_case(col_))\n",
    "traffic_detectors.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f32a0",
   "metadata": {},
   "source": [
    "## Individual Address Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc56791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------------------+-----------------+--------------+\n",
      "|           record_id|     host_read_time|field_device_read_time|reader_identifier|device_address|\n",
      "+--------------------+-------------------+----------------------+-----------------+--------------+\n",
      "|e60fa504f56592882...|2019-08-28 08:08:15|   2019-08-28 08:08:24|     lamar_morrow|          5444|\n",
      "|6ccf9bc09f94adcf6...|2019-08-28 08:08:25|   2019-08-28 08:08:35|     lamar_morrow|         37197|\n",
      "|8c92a42d206a0c867...|2019-08-28 08:08:36|   2019-08-28 08:08:46|     lamar_morrow|          7947|\n",
      "|b141f5991a32e5874...|2019-08-28 08:09:08|   2019-08-28 08:09:18|     lamar_morrow|          1701|\n",
      "|9a945c4a0553c1e4e...|2019-08-28 08:09:12|   2019-08-28 08:09:21|     lamar_morrow|           278|\n",
      "|0c009feda4adb5440...|2019-08-28 08:09:19|   2019-08-28 08:09:29|     lamar_morrow|         37198|\n",
      "|26ab5aaf898ed8da9...|2019-08-28 08:09:24|   2019-08-28 08:09:34|     lamar_morrow|         24651|\n",
      "|d8c8c7546ea49142d...|2019-08-28 08:09:40|   2019-08-28 08:09:50|     lamar_morrow|          1936|\n",
      "|4a47e0c629bb22822...|2019-08-28 08:09:51|   2019-08-28 08:10:01|     lamar_morrow|          7633|\n",
      "|d990f636eb5e18f38...|2019-08-28 08:10:01|   2019-08-28 08:10:11|     lamar_morrow|         18157|\n",
      "+--------------------+-------------------+----------------------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "IAFs = spark.read.option(\"header\", True).csv(\n",
    "    f\"{base_path}/Bluetooth_Travel_Sensors_-_Individual_Address_Files__IAFs_.csv\"\n",
    ")\n",
    "for col_ in IAFs.columns:\n",
    "    IAFs = IAFs.withColumnRenamed(col_, to_snake_case(col_))\n",
    "# Convert string to timestamp\n",
    "IAFs = IAFs.withColumn(03/11/2020 02:30:00 PM\n",
    "    \"host_read_time\", to_timestamp(col(\"host_read_time\"), \"MM/dd/yyyy hh:mm:ss a\")\n",
    ")\n",
    "IAFs = IAFs.withColumn(\n",
    "    \"field_device_read_time\",\n",
    "    to_timestamp(col(\"field_device_read_time\"), \"MM/dd/yyyy hh:mm:ss a\"),\n",
    ")\n",
    "\n",
    "\n",
    "IAFs.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59086113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructField('record_id', StringType(), False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StructField(\"record_id\", StringType(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ada2fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412529530"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IAFs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5d2478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412529530"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IAFs.select(\"record_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ade3ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11211525"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IAFs.select(\"device_address\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5749c1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IAFs.select(\"reader_identifier\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2be4ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host_read_time            08/28/2019 08:08:15 AM\n",
       "field_device_read_time    08/28/2019 08:08:24 AM\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = IAFs.select(\"host_read_time\", \"field_device_read_time\").limit(10).toPandas()\n",
    "temp.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb768af0",
   "metadata": {},
   "source": [
    "## Individual Traffic Match Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "480f3919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------------------------+-----------------------------+-------------------+--------------------+--------------+-----------------+-------------------+-------------------+-----------+\n",
      "|           record_id|device_address|origin_reader_identifier|destination_reader_identifier|travel_time_seconds|speed_miles_per_hour|match_validity|filter_identifier|         start_time|           end_time|day_of_week|\n",
      "+--------------------+--------------+------------------------+-----------------------------+-------------------+--------------------+--------------+-----------------+-------------------+-------------------+-----------+\n",
      "|cdac7d191483cacf2...|           706|              lamar_45th|                   lamar_38th|                 50|                  47|         valid|              125|2019-08-17T23:28:56|2019-08-17T23:29:46|   Saturday|\n",
      "|6e2a448e185b9742c...|           707|              lamar_45th|                   lamar_38th|                 58|                  41|         valid|              125|2019-08-17T23:33:12|2019-08-17T23:34:10|   Saturday|\n",
      "|32e7a2cd3371ac27c...|             3|              lamar_45th|                   lamar_38th|                 67|                  35|         valid|              125|2019-08-17T23:35:47|2019-08-17T23:36:54|   Saturday|\n",
      "|c34fc75428e9cb3c3...|           708|              lamar_45th|                   lamar_38th|                 44|                  54|       invalid|              125|2019-08-17T23:47:02|2019-08-17T23:47:46|   Saturday|\n",
      "|c80a810642d75887a...|           709|              lamar_29th|                 lamar_morrow|                793|                  17|       invalid|              125|2019-08-16T23:55:46|2019-08-17T00:08:59|     Friday|\n",
      "|ce42f461498764d57...|           710|              lamar_29th|                 lamar_morrow|               3701|                   4|       invalid|              125|2019-08-16T23:53:12|2019-08-17T00:54:53|     Friday|\n",
      "|ef4576f8dc6a94c91...|           711|              lamar_29th|                 lamar_morrow|               2690|                   5|       invalid|              125|2019-08-17T00:11:28|2019-08-17T00:56:18|   Saturday|\n",
      "|8e5cce54a3cf2a8ee...|           712|              lamar_29th|                 lamar_morrow|               2464|                   6|       invalid|              125|2019-08-17T02:30:02|2019-08-17T03:11:06|   Saturday|\n",
      "|0464beae0558de80c...|           713|              lamar_29th|                 lamar_morrow|               1566|                   9|       invalid|              125|2019-08-17T03:05:26|2019-08-17T03:31:32|   Saturday|\n",
      "|71c4422255cf52f63...|           714|              lamar_29th|                 lamar_morrow|                409|                  33|       invalid|              125|2019-08-17T03:33:50|2019-08-17T03:40:38|   Saturday|\n",
      "+--------------------+--------------+------------------------+-----------------------------+-------------------+--------------------+--------------+-----------------+-------------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ITMF = spark.read.option(\"header\", True).csv(\n",
    "    f\"{base_path}/Bluetooth_Travel_Sensors_-_Individual_Traffic_Match_Files__ITMF_.csv\"\n",
    ")\n",
    "for col_ in ITMF.columns:\n",
    "    ITMF = ITMF.withColumnRenamed(col_, to_snake_case(col_))\n",
    "ITMF.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ea163",
   "metadata": {},
   "source": [
    "## Traffic Match Summary Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d8ef12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+-----------------------------+--------------+-------------------+----------------+-------------------+------------------------+---------------------+--------------------+--------------------+---------------------------+-----------------+------------------------+--------------+------------------+\n",
      "|           record_id|origin_reader_identifier|destination_reader_identifier|origin_roadway|origin_cross_street|origin_direction|destination_roadway|destination_cross_street|destination_direction|segment_length_miles|           timestamp|average_travel_time_seconds|average_speed_mph|summary_interval_minutes|number_samples|standard_deviation|\n",
      "+--------------------+------------------------+-----------------------------+--------------+-------------------+----------------+-------------------+------------------------+---------------------+--------------------+--------------------+---------------------------+-----------------+------------------------+--------------+------------------+\n",
      "|2fd681965926a3046...|               lamar_5th|                   lamar_24th|         Lamar|                5th|      Northbound|              Lamar|                    24th|           Southbound|                1.35|03/10/2021 02:00:...|                         -1|               -1|                      15|             0|                -1|\n",
      "|7ee59a7d82171086f...|               lamar_5th|                    lamar_6th|         Lamar|                5th|      Northbound|              Lamar|                  6th St|           Southbound|               0.095|03/10/2021 02:00:...|                         -1|               -1|                      15|             0|                -1|\n",
      "|b7566c82386e4279a...|               lamar_5th|         lamar_barton_springs|         Lamar|                5th|      Southbound|              Lamar|          Barton Springs|           Northbound|               0.691|03/10/2021 02:00:...|                         -1|               -1|                      15|             0|                -1|\n",
      "|03c6f6398262d3035...|               lamar_5th|              lamar_riverside|         Lamar|                5th|      Southbound|              Lamar|               Riverside|           Northbound|               0.426|03/10/2021 02:00:...|                         -1|               -1|                      15|             0|                -1|\n",
      "|e59562cddbd6fadb1...|               lamar_6th|                 6th_campbell|           6th|              Lamar|       Westbound|                6th|                Campbell|            Eastbound|               0.679|03/10/2021 02:00:...|                         -1|               -1|                      15|             0|                -1|\n",
      "|0742f197c5ec793b7...|               lamar_6th|                   lamar_12th|         Lamar|                6th|      Northbound|              Lamar|                    12th|           Southbound|               0.398|03/10/2021 02:00:...|                         -1|               -1|                      15|             0|                -1|\n",
      "|e1f39a994b793e78d...|               lamar_6th|                    lamar_5th|         Lamar|                6th|      Southbound|              Lamar|                     5th|           Northbound|               0.095|03/10/2021 02:00:...|                         -1|               -1|                      15|             0|                -1|\n",
      "|c2f3e5a6b4440a06b...|               lamar_6th|                    lamar_mlk|         Lamar|                6th|      Northbound|              Lamar|                     MLK|           Southbound|                 0.9|03/10/2021 02:00:...|                        122|               27|                      15|             5|              0.55|\n",
      "|240c195953d9b3e45...|               lamar_6th|              lamar_riverside|         Lamar|                6th|      Southbound|              Lamar|               Riverside|           Northbound|             0.52278|03/10/2021 02:00:...|                         80|               24|                      15|             3|              5.57|\n",
      "|5cee0e149b0712308...|           lamar_airport|                   lamar_38th|         Lamar|            Airport|      Southbound|              Lamar|                    38th|           Northbound|                2.71|03/10/2021 02:00:...|                        381|               26|                      15|             2|              6.36|\n",
      "+--------------------+------------------------+-----------------------------+--------------+-------------------+----------------+-------------------+------------------------+---------------------+--------------------+--------------------+---------------------------+-----------------+------------------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TMSR = spark.read.option(\"header\", True).csv(\n",
    "    f\"{base_path}/Bluetooth_Travel_Sensors_-Traffic_Match_Summary_Records__TMSR__20250125.csv\"\n",
    ")\n",
    "for col_ in TMSR.columns:\n",
    "    TMSR = TMSR.withColumnRenamed(col_, to_snake_case(col_))\n",
    "TMSR.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a306d832-47bf-403e-becb-33d2e66a354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
